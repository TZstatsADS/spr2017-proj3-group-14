Also to systematically solve the problem of overfitting:
*(High dimensionality)


1. (Variable selection)

  1.1 (Best subset selection)
  
  1.2 (stepwise selection)
  
2. (Regularization)
  
  2.1 Ridge
  
  2.2 LASSO (see attached fig, rule out this method)
  
  2.3 SCAD
  
  2.4 elastic net

3. (Dimension reduction techniques) (see attached fig, rule out this method)

  3.1 (Principal component regression) 

  3.2 (Factor analysis)
  
  3.3 (Partial least squares)





Use lfda to dimension reduction and svm to predict

```{r}
library(lfda)
library(plotly)
library(MASS)
library(matrixStats)
library(caret)
library(e1071)

sift= fread("sift_features.csv", header= F)
sift.df= as.data.frame(t(sift))

labels<-read.csv("labels.csv")
label.train <- as.data.frame(labels)
names(label.train) <- c("val")
label.train$val <- as.factor(label.train$val)

data.all = as.data.frame(apply(data.all,2, as.numeric))
data.all$val = as.factor(data.all$val)


#split into training and testing
set.seed(2)
train=sample(1:nrow(data.all),nrow(data.all)*0.8)
test=-train
dat.train=data.all[train,-1]
label_test=label.train[test]
test_data=data.all[test,-1]
test_label=data.all$V1[test]
```

```{r, echo=FALSE, out.width=12, fig.align="center", fig.width=12}
lowVariance <- nearZeroVar(dat.train)
dat.train.variance <- dat.train[,-lowVariance]
good.variance.ncol <- ncol(dat.train.variance)
numcol.to.use <- min(good.variance.ncol, nrow(dat.train.variance))-100


fda.model <- lfda(x = dat.train[,1:numcol.to.use], y = label.train, r = numcol.to.use, metric="plain")
Z <- as.data.frame(fda.model$Z)

```

Error levels of the 10-fold cross validation
```{r, echo=FALSE}
z.labeled <- cbind(Z, label.train)

svm.model <- svm(label.train ~ ., 
                 data = z.labeled,
                 cross = 10)

svm.model$accuracies
# [1] 52.500 55.000 49.375 50.000 50.000 47.500 47.500 50.000 51.875 51.875
```

### 6. SVM over the FDA with just 2 fisher features 
Error levels of the 10-fold cross validation
```{r, echo=FALSE}
z.labeled.fewCols <- cbind(Z[,1:2], label.train)

svm.model.few <- svm( ~ ., 
                 data = z.labeled.fewCols,
                 cross = 10)

svm.model.few$accuracies
#[1] 55.625 50.000 46.250 51.250 47.500 55.000 54.375 50.625 43.125 43.750

```


#library
library(adabag)

#load data 

features=read.csv("sift_features.csv")
labels=read.csv("labels.csv")
features=t(features)


all_data<-data.frame(labels,features)
all_data$V1=as.factor(all_data$V1)
dim(all_data)

set.seed(10)
positions <- sample(nrow(all_data),size=floor((nrow(all_data)/4)*3))
training<- all_data[positions,]
testing<- all_data[-positions,]



#train
boostingmodel<- boosting(V1 ~ ., data = training, mfinal = 10, control = rpart.control(maxdepth = 1))
#error
table(boostingmodel$class, training$V1,dnn = c("Predicted Class", "Observed Class"))
1 - sum(boostingmodel$class == training$V1) /length(training$V1)
#0.31


#testing error
pred=predict.boosting(boostingmodel,newdata = testing[,-1])
sum(testing[,1]!=pred$class)
#0.338


#boosting cv
boostingcv=boosting.cv(V1 ~ ., v = 10, data = all_data, mfinal = 10,control = rpart.control(maxdepth = 1))
#0.351 error




#bagging of different methods
#pred of boosting
#random forest
library(randomForest)
rf_fit<-randomForest(training$V1 ~.,data=training,ntree=500)
rf_pred=predict(rf_fit,newdata=testing)
sum(testing[,1]!=rf_pred)/500

#svm all of them are 1!!
library(e1071)
svm_fit<-svm(training$V1~.,data=training)
svm_pred=predict(svm_fit,newdata=testing)

#logit no converge
lm_fit<-glm(training$V1~.,family=binomial(link='logit'),data=training)
lm_pred=ifelse(predict(lm_fit,newdata=testing)>-75,1,0)


x=data.frame(svm_pred,rf_pred,lm_pred,pred$class,testing[,1])
write.csv(x,"output.csv")
xx=data.frame(rf_pred,lm_pred,pred$class)
#the best can be 0.278





